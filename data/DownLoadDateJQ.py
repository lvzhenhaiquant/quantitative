
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import os
import sys
import jqdatasdk as jq
import json
from urllib3.exceptions import NameResolutionError, MaxRetryError, ConnectionError
import requests
import pandas as pd
from tqdm import tqdm
import time
from typing import Dict,List

from DataBase_ClickHouse import ClickHouseDB


class DownloadDataFromJointQuant:
    def __init__(self):
        self.config_path = './config.json'
        self.MAX_RETRY = 10  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.save_dir_index_weight = './download_Jqdata/index_weight'

        self.database_name = 'quantitative'
        self.table_name_A_basic_df = 'A_basic_df'
        self.table_name_A_basic_1min_df = 'A_basic_1min_df'
        self.table_name_A_basic_5min_df = 'A_basic_5min_df'
        self.table_name_A_basic_15min_df = 'A_basic_15min_df'
        self.table_name_A_basic_30min_df = 'A_basic_30min_df'
        self.table_name_A_basic_60min_df = 'A_basic_60min_df'

        self.table_name_A_daily_basic_df = 'A_daily_basic_df'
        self.table_name_index_daily_basic_df = 'index_daily_basic_df'

        self.table_name_index_daily_df = 'index_daily_df'
        self.table_name_index_daily_1min_df = 'index_daily_1min_df'
        self.table_name_index_daily_5min_df = 'index_daily_5min_df'
        self.table_name_index_daily_15min_df = 'index_daily_15min_df'
        self.table_name_index_daily_30min_df = 'index_daily_30min_df'
        self.table_name_index_daily_60min_df = 'index_daily_60min_df'

        self.table_name_shenwan_index_basic_df = 'shenwan_index_basic_df'
        self.table_name_shewan_index_basic_1min_df = 'shenwan_index_basic_1min_df'
        self.table_name_shewan_index_basic_5min_df = 'shenwan_index_basic_5min_df'
        self.table_name_shewan_index_basic_15min_df = 'shenwan_index_basic_15min_df'
        self.table_name_shewan_index_basic_30min_df = 'shenwan_index_basic_30min_df'
        self.table_name_shewan_index_basic_60min_df = 'shenwan_index_basic_60min_df'

        self.table_name_index_constituent_stock_df = 'index_constituent_stock_df'
        self.table_name_shenwan_index_constituent_stock_df = 'shenwan_index_constituent_stock_df'
        self.table_name_indicator = 'indicator_df'
        self.table_name_balance = 'balance_df'
        self.table_name_income = 'income_df'
        self.table_name_cashflow = 'cashflow_df'

        self.table_name_sw_l1 = 'sw_l1'
        self.table_name_sw_l2 = 'sw_l2'
        self.table_name_sw_l3 = 'sw_l3'

        self.table_name_index_daily = 'download_index_daily_df'
        self.table_name_shenwan_daily = 'download_shenwan_daily_df'
        self.table_name_shenwan_stock_industry_df = 'shenwan_stock_industry_df'



        self.index_mapping = {
            "ä¸Šè¯50":    "000016.XSHG",
            "æ²ªæ·±300":   "000300.XSHG",
            "ç§‘åˆ›50":    "000688.XSHG", 
            "ä¸­è¯1000":  "000852.XSHG",  
            "ä¸­è¯500":   "000905.XSHG",   
            "ä¸­è¯800":   "000906.XSHG",  
            "ä¸­è¯å…¨æŒ‡":  "000985.XSHG", 
            "åˆ›ä¸šæ¿æŒ‡":  "399006.XSHE" 
        }
        self.code_to_csi = {
            "000016.XSHG": "csi50",  # ä¸Šè¯50
            "000300.XSHG": "csi300",  # æ²ªæ·±300
            "000688.XSHG": "csi50kechuang",  # ç§‘åˆ›50
            "000852.XSHG": "csi1000",  # ä¸­è¯1000
            "000905.XSHG": "csi500",  # ä¸­è¯500
            "000906.XSHG": "csi800",  # ä¸­è¯800
            "000985.XSHG": "csiall",  # ä¸­è¯å…¨æŒ‡
            "399006.XSHE": "csigg",  # åˆ›ä¸šæ¿æŒ‡
        }
        self.init_jointquant()

        self.db = ClickHouseDB()
        if not self.db.connect_flag:
            print("æ•°æ®åº“è¿æ¥å¤±è´¥")
            return

    
    def init_jointquant(self):
        config = self._load_config()
        jq_username = config.get('jq_username', '') if config else ''
        jq_password = config.get('jq_password', '') if config else ''
        if not jq_username or not jq_password:
            raise ValueError("JointQuantç”¨æˆ·åæˆ–å¯†ç æœªé…ç½®")
        try:
            jq.auth(jq_username, jq_password)
            print("èšå®½æ•°æ®ç™»å½•æˆåŠŸ")
        except Exception as e:
            raise ValueError(f"èšå®½æ•°æ®ç™»å½•å¤±è´¥ï¼š{str(e)}")
            
        

    def _load_config(self):      
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                # è·å–clickhouseéƒ¨åˆ†çš„é…ç½®
                if 'jointquant' in config_data:
                    return config_data['jointquant']
                else:
                    return config_data
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶ {self.config_path} æ—¶å‡ºé”™: {e}")
            return {}

    
    def download_A_basic(self,start_date_str,end_date_str):
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸ºdatetime
        if isinstance(start_date_str, str):
            start_date_str = datetime.strptime(start_date_str, '%Y-%m-%d')
        if isinstance(end_date_str, str):
            end_date_str = datetime.strptime(end_date_str, '%Y-%m-%d')

        all_stocks = self.get_all_stocks()[:3] # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 

        index_codes = list(self.index_mapping.values()) # è·å–æ‰€æœ‰æŒ‡æ•°ä»£ç 

        shenwan_index_codes = self._get_all_shenwan_index_codes() # è·å–ç”³ä¸‡æŒ‡æ•°ä»£ç 

        #è·å–ç”³ä¸‡æŒ‡æ•°ä»£ç 

        if len(all_stocks) == 0:
            print("æ— æœ‰æ•ˆæˆåˆ†è‚¡æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
            return
        
        with ThreadPoolExecutor(max_workers=1) as executor:
            # æäº¤ä»»åŠ¡
            basic_df_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'daily',self.table_name_A_basic_df,['trade_date','code'])# è‚¡ç¥¨åŸºç¡€æ•°æ® ok
            basic_df_1min_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'1m',self.table_name_A_basic_1min_df,['trade_date','code'])# è‚¡ç¥¨1åˆ†é’Ÿçº§æ•°æ® ok
            basic_df_5min_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'5m',self.table_name_A_basic_5min_df,['trade_date','code'])# è‚¡ç¥¨5åˆ†é’Ÿçº§æ•°æ® ok
            basic_df_15min_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'15m',self.table_name_A_basic_15min_df,['trade_date','code'])# è‚¡ç¥¨15åˆ†é’Ÿçº§æ•°æ® ok
            basic_df_30min_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'30m',self.table_name_A_basic_30min_df,['trade_date','code'])# è‚¡ç¥¨30åˆ†é’Ÿçº§æ•°æ® ok
            basic_df_60min_result = executor.submit(self._get_daily, all_stocks, start_date_str, end_date_str,'60m',self.table_name_A_basic_60min_df,['trade_date','code'])# è‚¡ç¥¨60åˆ†é’Ÿçº§æ•°æ®  ok

            basic_df_daily_result = executor.submit(self._get_daily_basic, "stock", all_stocks, start_date_str, end_date_str,self.table_name_A_daily_basic_df,['trade_date','code'])# è‚¡ç¥¨æ—¥çº¿basicæ•°æ® ok
            index_basic_df_result = executor.submit(self._get_daily_basic, "index", index_codes, start_date_str, end_date_str,self.table_name_index_daily_basic_df,['trade_date','code'])# æŒ‡æ•°æ—¥çº¿basicæ•°æ® ok


            index_daily_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'daily',self.table_name_index_daily_df,['trade_date','code']) # æŒ‡æ•°æ—¥çº¿è¡Œæƒ…  ok
            index_daily_1min_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'1m',self.table_name_index_daily_1min_df,['trade_date','code']) # æŒ‡æ•°1åˆ†é’Ÿçº§æ•°æ® ok
            index_daily_5min_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'5m',self.table_name_index_daily_5min_df,['trade_date','code']) # æŒ‡æ•°5åˆ†é’Ÿçº§æ•°æ® ok
            index_daily_15min_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'15m',self.table_name_index_daily_15min_df,['trade_date','code']) # æŒ‡æ•°15åˆ†é’Ÿçº§æ•°æ® ok
            index_daily_30min_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'30m',self.table_name_index_daily_30min_df,['trade_date','code']) # æŒ‡æ•°30åˆ†é’Ÿçº§æ•°æ® ok
            index_daily_60min_df_result = executor.submit(self._get_daily,index_codes, start_date_str, end_date_str,'60m',self.table_name_index_daily_60min_df,['trade_date','code']) # æŒ‡æ•°60åˆ†é’Ÿçº§æ•°æ®   ok

            index_constituent_stock_df_result = executor.submit(self._get_index_constituent_stock, start_date_str, end_date_str) # æŒ‡æ•°æˆåˆ†è‚¡  ok
            # shewan_index_constituent_stock_df_result = executor.submit(self._get_shenwan_index_constituent_stock, shenwan_index_codes,start_date_str, end_date_str) # ç”³ä¸‡æŒ‡æ•°æˆåˆ†è‚¡
            shenwan_classify_df_result = executor.submit(self._get_shenwan_classify) # ç”³ä¸‡åˆ†ç±»æ•°æ®ï¼Œè¡Œä¸šåˆ†ç±»ï¼Œè‚¡ç¥¨-è¡Œä¸šåˆ†ç±» ok

            fundamentals_df_result = executor.submit(self._get_history_fundamentals, all_stocks, start_date_str,end_date_str,"indicator",self.table_name_indicator,["code","statDate","pubDate"])# indicator æ•°æ®  ok
            balance_df_result = executor.submit(self._get_history_fundamentals, all_stocks, start_date_str,end_date_str,"balance",self.table_name_balance,["code","statDate","pubDate"])# è´¢åŠ¡æ•°æ®  ok
            income_df_result = executor.submit(self._get_history_fundamentals, all_stocks, start_date_str,end_date_str,"income",self.table_name_income,["code","statDate","pubDate"])# è´¢åŠ¡æ•°æ®  ok
            cashflow_df_result = executor.submit(self._get_history_fundamentals, all_stocks, start_date_str,end_date_str,"cashflow",self.table_name_cashflow,["code","statDate","pubDate"])# è´¢åŠ¡æ•°æ®  ok

            # è·å–ç»“æœ

    def _get_daily(self, stock_list, start_date_str, end_date_str, frequency_str, table_name, primary_key_list):
        basic_all = []
        basic_df = pd.DataFrame()
        max_retry = self.MAX_RETRY  # æœ€å¤§é‡è¯•æ¬¡æ•°
        # æ ¹æ®é¢‘ç‡ç¡®å®šæ‰¹æ¬¡å¤§å°
        if frequency_str == 'daily':
            batch_days = 5000  # 5000æ¡ï¼Œ5000å¤©
        elif frequency_str == '1m':
            batch_days = 18  # 18å¤© 
        elif frequency_str == '5m':
            batch_days = 80  # 80å¤©
        elif frequency_str == '15m':
            batch_days = 200  # 10å¤©
        elif frequency_str == '30m':
            batch_days = 400  # 400å¤©
        elif frequency_str == '60m':
            batch_days = 800  # 800å¤©
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢‘ç‡: {frequency_str}")
        for i, stock_code in enumerate(tqdm(stock_list, desc=f'è·å–æ—¥çº¿æ•°æ®{frequency_str}'), 1):
            retry_count = 0
            success = False
            tmp = None
            retry_delay = 1  # æ¯åªè‚¡ç¥¨é‡ç½®åˆå§‹é‡è¯•é—´éš”ä¸º1ç§’
            # å°†æ—¥æœŸèŒƒå›´æ‹†åˆ†ä¸ºå°æ‰¹æ¬¡
            current_start = pd.to_datetime(start_date_str)
            end_date = pd.to_datetime(end_date_str)
            while current_start <= end_date:
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸæ—¥æœŸ
                current_end = min(current_start + pd.DateOffset(days=batch_days), end_date)
                current_start_str = current_start.strftime('%Y-%m-%d')
                current_end_str = current_end.strftime('%Y-%m-%d')
                retry_count = 0  # æ¯ä¸ªæ‰¹æ¬¡é‡ç½®é‡è¯•è®¡æ•°
                success = False
                while retry_count < max_retry and not success:
                    try:
                        # æŸ¥è¯¢è¯¥è‚¡ç¥¨åœ¨å½“å‰æ‰¹æ¬¡æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                        tmp = jq.get_price(security=stock_code, start_date=current_start_str, end_date=current_end_str, frequency=frequency_str, skip_paused=False, fq='post', count=None, round=True)
                        success = True  # æˆåŠŸè·å–ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                        if len(tmp) > 0:
                            tmp['code'] = stock_code
                            tmp = tmp.reset_index(names='trade_date')
                            if frequency_str == 'daily':
                                tmp['trade_date'] = tmp['trade_date'].dt.strftime('%Y-%m-%d')
                            else:
                                tmp['trade_date'] = pd.to_datetime(tmp['trade_date']).dt.tz_localize(None)
                            self.save_data_to_database_with_threadpool(tmp, primary_key_list, table_name)
                            basic_all.append(tmp)
                        else:
                            print(f"daily_basic_ æœªæŸ¥è¯¢åˆ°è‚¡ç¥¨{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®")
                        
                    # åªæ•è·ç½‘ç»œç›¸å…³å¼‚å¸¸ï¼Œéç½‘ç»œå¼‚å¸¸ç›´æ¥è·³è¿‡é‡è¯•
                    except Exception as e:
                        retry_count += 1
                        if retry_count < max_retry:
                            print(f"_get_daily è·å–{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰ï¼š{str(e)[:50]}... ç¬¬{retry_count}æ¬¡é‡è¯•ï¼Œç­‰å¾…{retry_delay}ç§’")
                            time.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿ï¼Œé—´éš”ç¿»å€
                        else:
                            print(f"_get_daily è·å–{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®å¤±è´¥ï¼š{str(e)[:50]}... å·²é‡è¯•{max_retry}æ¬¡ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
                current_start = current_end + pd.DateOffset(days=1)
                # é‡ç½®å»¶è¿Ÿæ—¶é—´
                retry_delay = 1
        # åˆå¹¶ç»“æœ
        if basic_all:
            basic_all = [df for df in basic_all if not df.empty]
            basic_df = pd.concat(basic_all, ignore_index=True)
            print(f"\nå…±è·å–{len(basic_df)}æ¡æ•°æ®")
        return basic_df

    def _get_history_fundamentals(self, stock_list, stat_date_str, end_date_str, table_name_jq, table_name_local,primary_key_list):
        exclude_fields = ['id', 'day']  # å¯æ ¹æ®éœ€æ±‚æ‰©å±•
        table_info = jq.get_table_info(table_name_jq)
        field_names = table_info['name_en'].tolist()
        field_names = [f for f in field_names if f not in exclude_fields]
        field_names = [f"{table_name_jq}.{f}" for f in field_names]
        max_retry = self.MAX_RETRY  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
        start_year = stat_date_str.year
        end_year = end_date_str.year
        years = list(range(start_year, end_year + 1))  # æŒ‰å¹´éå†çš„åˆ—è¡¨
        
        for i, stock_code in enumerate(tqdm(stock_list, desc=f'è·å–è´¢åŠ¡æ•°æ®{table_name_jq}'), 1):
            stock_data_list = []  # å­˜å‚¨å•åªè‚¡ç¥¨æ‰€æœ‰å¹´ä»½çš„æ•°æ®
            retry_delay = 1       # æ¯åªè‚¡ç¥¨é‡ç½®åˆå§‹é‡è¯•é—´éš”ä¸º1ç§’
            
            # æŒ‰å¹´ä»½å¾ªç¯æ‹‰å–
            for year in years:
                retry_count = 0
                success = False
                tmp = None
                while retry_count < max_retry and not success:
                    try:
                        # æ„é€ å½“å¹´çš„æŸ¥è¯¢æ—¥æœŸï¼ˆå¦‚2023å¹´æŸ¥2023-12-31ï¼‰
                        year_watch_date = f"{year}-12-31"
                        # æŸ¥è¯¢è¯¥è‚¡ç¥¨å½“å¹´çš„è´¢åŠ¡æ•°æ®ï¼ˆæŒ‰å­£åº¦æ‹‰å–ï¼Œcount=4è¡¨ç¤º4ä¸ªå­£åº¦ï¼‰
                        tmp = jq.get_history_fundamentals(
                            security=stock_code,
                            watch_date=year_watch_date,  # æŒ‰å¹´ä»½æŸ¥è¯¢
                            fields=field_names,
                            count=4,                     # æ‹‰å–å½“å¹´4ä¸ªå­£åº¦æ•°æ®
                            interval='1q',
                            stat_by_year=False
                        )
                        success = True  # æˆåŠŸè·å–ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                        if not tmp.empty:
                            stock_data_list.append(tmp)
                            self.save_data_to_database_with_threadpool(tmp, primary_key_list, table_name_local)
                        else:
                            print(f"æœªæŸ¥è¯¢åˆ°è‚¡ç¥¨{stock_code} {year}å¹´çš„{table_name_jq}æ•°æ®")
                            
                    except (NameResolutionError, MaxRetryError, ConnectionError, TimeoutError,ConnectionResetError, requests.exceptions.RequestException) as e:
                        retry_count += 1
                        if retry_count < max_retry:
                            print(f"è·å–{stock_code} {year}å¹´{table_name_jq}æ•°æ®å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰ï¼š{str(e)[:50]}... ç¬¬{retry_count}æ¬¡é‡è¯•ï¼Œç­‰å¾…{retry_delay}ç§’")
                            time.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿ï¼Œé—´éš”ç¿»å€
                        else:
                            print(f"è·å–{stock_code} {year}å¹´{table_name_jq}æ•°æ®å¤±è´¥ï¼š{str(e)[:50]}... å·²é‡è¯•{max_retry}æ¬¡ï¼Œè·³è¿‡è¯¥å¹´ä»½æ•°æ®")
                            break
                    except Exception as e:
                        print(f"è·å–{stock_code} {year}å¹´{table_name_jq}æ•°æ®å¤±è´¥ï¼ˆéç½‘ç»œé”™è¯¯ï¼‰ï¼š{e}ï¼Œè·³è¿‡è¯¥å¹´ä»½æ•°æ®")
                        break  # è·³å‡ºwhileé‡è¯•å¾ªç¯
            if stock_data_list:
                stock_data = pd.concat(stock_data_list, ignore_index=True)
            else:
                print(f"_get_history_fundamentals æœªæŸ¥è¯¢åˆ°è‚¡ç¥¨{stock_code} {start_year}-{end_year}å¹´çš„{table_name_jq}æ•°æ®")

    def _get_daily_basic(self, stock_type:str, stock_list, start_date_str, end_date_str, table_name, primary_key_list):
        basic_all = []
        basic_df = pd.DataFrame()
        max_retry = self.MAX_RETRY  # æœ€å¤§é‡è¯•æ¬¡æ•°
        for i, stock_code in enumerate(tqdm(stock_list, desc='è·å–æ—¥çº¿æ•°æ®'), 1):
            retry_count = 0
            success = False
            tmp = None
            retry_delay = 1  # æ¯åªè‚¡ç¥¨é‡ç½®åˆå§‹é‡è¯•é—´éš”ä¸º1ç§’
            # å°†æ—¥æœŸèŒƒå›´æ‹†åˆ†ä¸ºå°æ‰¹æ¬¡
            current_start = pd.to_datetime(start_date_str)
            end_date = pd.to_datetime(end_date_str)
            while current_start <= end_date:
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸæ—¥æœŸ
                current_end = min(current_start + pd.DateOffset(days=90), end_date)
                current_start_str = current_start.strftime('%Y-%m-%d')
                current_end_str = current_end.strftime('%Y-%m-%d')
                retry_count = 0  # æ¯ä¸ªæ‰¹æ¬¡é‡ç½®é‡è¯•è®¡æ•°
                success = False
                while retry_count < max_retry and not success:
                    try:
                        # æŸ¥è¯¢è¯¥è‚¡ç¥¨åœ¨å½“å‰æ‰¹æ¬¡æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                        if stock_type =="stock":
                            tmp = jq.get_valuation(security_list=stock_code, start_date=current_start_str, end_date=current_end_str)
                        elif stock_type == "index":
                            tmp = jq.get_index_valuation(security_list=stock_code, start_date=current_start_str, end_date=current_end_str)
                        else:
                            raise ValueError(f"_get_daily_basic æœªçŸ¥çš„è‚¡ç¥¨ç±»å‹{stock_type}")
                        tmp.rename(columns={"day": "trade_date"}, inplace=True)
                        success = True  # æˆåŠŸè·å–ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                        if len(tmp) > 0:
                            self.save_data_to_database_with_threadpool(tmp, primary_key_list, table_name)
                            basic_all.append(tmp)
                        else:
                            print(f"daily_basic_ æœªæŸ¥è¯¢åˆ°è‚¡ç¥¨{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®")
                    # åªæ•è·ç½‘ç»œç›¸å…³å¼‚å¸¸ï¼Œéç½‘ç»œå¼‚å¸¸ç›´æ¥è·³è¿‡é‡è¯•
                    except (NameResolutionError, MaxRetryError, ConnectionError, TimeoutError, ConnectionResetError, requests.exceptions.RequestException) as e:
                        retry_count += 1
                        if retry_count < max_retry:
                            print(f"_get_daily_basic è·å–{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰ï¼š{str(e)[:50]}... ç¬¬{retry_count}æ¬¡é‡è¯•ï¼Œç­‰å¾…{retry_delay}ç§’")
                            time.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿ï¼Œé—´éš”ç¿»å€
                        else:
                            print(f"_get_daily_basic è·å–{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®å¤±è´¥ï¼š{str(e)[:50]}... å·²é‡è¯•{max_retry}æ¬¡ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                    except Exception as e:
                        print(f"_get_daily_basic è·å–{stock_code}åœ¨{current_start_str}è‡³{current_end_str}çš„æ•°æ®å¤±è´¥ï¼ˆéç½‘ç»œé”™è¯¯ï¼‰ï¼š{e}ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                        break  # è·³å‡ºwhileé‡è¯•å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
                current_start = current_end + pd.DateOffset(days=1)
                # é‡ç½®å»¶è¿Ÿæ—¶é—´
                retry_delay = 1
        # åˆå¹¶ç»“æœ
        if basic_all:
            basic_all = [df for df in basic_all if not df.empty]
            basic_df = pd.concat(basic_all, ignore_index=True)
            print(f"\nå…±è·å–{len(basic_df)}æ¡æ•°æ®")
        return basic_df


    def _get_index_constituent_stock(self,start_date_str,end_date_str):
        index_mapping = self.index_mapping
        code_to_csi = self.code_to_csi
        os.makedirs(self.save_dir_index_weight, exist_ok=True)
        for index_code in  tqdm(index_mapping.values(), desc='è·å–å„ç§æŒ‡æ•°æˆåˆ†è‚¡'):
            result_df = self._get_index_stocks(index_code,start_date_str,end_date_str)

            self.save_data_to_database_with_threadpool(result_df,['index_code','code','date'],self.table_name_index_constituent_stock_df)
            
            # æŒ‡æ•°åˆ—è¡¨æˆåˆ†è‚¡å•ç‹¬è½¬åˆ°jsonï¼Œæ–¹ä¾¿O(1)è¯»å–ä½¿ç”¨
            result_df = result_df.reset_index(drop=True)
            new_data = self._utils_convert_to_json(result_df)
            sorted_data = dict(sorted(new_data.items()))
            csi_name = code_to_csi[index_code]
            # ä¿å­˜ JSON æ–‡ä»¶
            filepath = os.path.join(self.save_dir_index_weight, f"{csi_name}.json")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sorted_data, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {filepath}")
        

    def _get_index_stocks(self, index_code, start_date_str, end_date_str):  # è·å–æŒ‡æ•°æˆåˆ†è‚¡
        """
        è·å–æŒ‡å®šæŒ‡æ•°åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æˆåˆ†è‚¡
        :param index_code: æŒ‡æ•°ä»£ç 
        :param start_date_str: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼å¦‚'2020-01-01'
        :param end_date_str: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼å¦‚'2020-12-31'
        :return: åŒ…å«index_code, code, dateåˆ—çš„DataFrame
        """
        # å‚æ•°éªŒè¯
        if not all([index_code, start_date_str, end_date_str]):
            print("âŒ å‚æ•°ç¼ºå¤±ï¼Œæ— æ³•è·å–æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")
            return pd.DataFrame(columns=['index_code', 'code', 'date'])
        current_start = pd.to_datetime(start_date_str)
        total_end = pd.to_datetime(end_date_str)
        index_list = []
        # æ£€æŸ¥æ—¥æœŸèŒƒå›´æœ‰æ•ˆæ€§
        if current_start > total_end:
            print("âŒ å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ")
            return pd.DataFrame(columns=['index_code', 'code', 'date'])
        # ç”Ÿæˆæ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨pandasçš„date_rangeæ›´é«˜æ•ˆ
        date_range = pd.date_range(start=current_start, end=total_end, freq='D')
        # æŒ‰å¤©é€æ‰¹éå†ï¼ˆæ ¸å¿ƒï¼šæ¯å¤©æŸ¥è¯¢ä¸€ä¸ªæ—¥æœŸçš„æ•°æ®ï¼‰
        for current_date in tqdm(date_range, desc=f'è·å–æŒ‡æ•° {index_code} æˆåˆ†è‚¡'):
            curr_date_str = current_date.strftime('%Y%m%d')
            try:
                temp_list = jq.get_index_stocks(index_symbol=index_code, date=curr_date_str)
                if temp_list and len(temp_list) > 0:
                    # æ‰¹é‡æ·»åŠ æ•°æ®ï¼Œå‡å°‘å¾ªç¯å†…æ“ä½œ
                    index_list.extend([
                        {'index_code': index_code, 'code': code, 'date': curr_date_str}
                        for code in temp_list  ])
            except Exception as e:
                print(f"âŒ è·å–{curr_date_str}æŒ‡æ•°åˆ—è¡¨å¤±è´¥ï¼š{e}")
        if index_list:
            index_df = pd.DataFrame(index_list)
            index_df['date'] = pd.to_datetime(index_df['date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')
        else:
            index_df = pd.DataFrame(columns=['index_code', 'code', 'date'])            
        return index_df


    def _get_shenwan_index_constituent_stock(self, shenwan_index_codes, start_date_str, end_date_str):
        """
        æ‰¹é‡è·å–ç”³ä¸‡æŒ‡æ•°æˆåˆ†è‚¡å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        :param shenwan_index_codes: ç”³ä¸‡æŒ‡æ•°ä»£ç åˆ—è¡¨
        :param start_date_str: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼å¦‚'2020-01-01'
        :param end_date_str: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼å¦‚'2020-12-31'
        """
        # å‚æ•°éªŒè¯
        if not shenwan_index_codes:
            print("âŒ æœªæä¾›ç”³ä¸‡æŒ‡æ•°ä»£ç åˆ—è¡¨")
            return
        all_results = []
        success_count = 0
        
        for index_code in tqdm(shenwan_index_codes, desc='è·å–ç”³ä¸‡æŒ‡æ•°æˆåˆ†è‚¡'):
            result_df = self._get_index_stocks(index_code, start_date_str, end_date_str)
            if result_df is not None and not result_df.empty:
                all_results.append(result_df)
                success_count += 1
            else:
                print(f"âš ï¸ æŒ‡æ•° {index_code} åœ¨ {start_date_str} åˆ° {end_date_str} æœŸé—´æ²¡æœ‰æˆåˆ†è‚¡æ•°æ®")
        # æ‰¹é‡å¤„ç†å’Œä¿å­˜æ•°æ®ï¼Œå‡å°‘æ•°æ®åº“è¿æ¥æ¬¡æ•°
        if all_results:
            combined_df = pd.concat(all_results, ignore_index=True)
            self.save_data_to_database_with_threadpool(
                combined_df,['index_code', 'code', 'date'],self.table_name_shenwan_index_constituent_stock_df)
        else:
            print(f"\nğŸ“Š æœªè·å–åˆ°ä»»ä½•ç”³ä¸‡æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")

    def get_all_stocks(self):
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨
            all_stocks = jq.get_all_securities(types=['stock'], date=None)
            # é‡ç½®ç´¢å¼•ï¼Œä½¿è‚¡ç¥¨ä»£ç æˆä¸ºæ™®é€šåˆ—
            all_stocks_reset = all_stocks.reset_index()
            all_stocks_reset.columns = ['code', 'display_name', 'name', 'start_date', 'end_date', 'type']
            return all_stocks_reset['code'].tolist()
        except Exception as e:
            print(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None
    
    def _get_all_shenwan_index_codes(self):
        """
        è·å–æ‰€æœ‰ç”³ä¸‡æŒ‡æ•°ä»£ç ï¼ˆä¸€çº§+äºŒçº§+ä¸‰çº§ï¼‰ï¼Œè¿”å›listæ ¼å¼
        :return: æ‰€æœ‰ç”³ä¸‡æŒ‡æ•°ä»£ç åˆ—è¡¨ ['index_code1', 'index_code2', ...]
        """
        try:
            sw_l1 = jq.get_industries(name='sw_l1').reset_index(names='index_code')
            sw_l2 = jq.get_industries(name='sw_l2').reset_index(names='index_code')
            sw_l3 = jq.get_industries(name='sw_l3').reset_index(names='index_code')
            sw_l1_codes = sw_l1['index_code'].tolist()
            sw_l2_codes = sw_l2['index_code'].tolist()
            sw_l3_codes = sw_l3['index_code'].tolist()
            all_shenwan_index_codes = list(set(sw_l1_codes + sw_l2_codes + sw_l3_codes))
            return all_shenwan_index_codes
        except Exception as e:
            print(f"âŒ è·å–ç”³ä¸‡æŒ‡æ•°ä»£ç å¤±è´¥ï¼š{str(e)[:100]}")
            return []  # å¼‚å¸¸æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­ä»£ç æŠ¥é”™

    def _utils_convert_to_json(self, df_weight: pd.DataFrame) -> Dict[str, List[str]]:
        """
        å°† DataFrame è½¬æ¢ä¸ºæ¯æ—¥æˆåˆ†è‚¡ JSON æ ¼å¼
        Args:
            df: index_weight è¿”å›çš„ DataFrame
        Returns:
            {date: [stock1, stock2, ...]}
        """
        if df_weight.empty:
            return {}
        df_weight = self.convert_jq_code_suffix(df_weight)
        # æŒ‰æ—¥æœŸåˆ†ç»„
        result = {}
        for trade_date, group in df_weight.groupby('date'):
            stocks = sorted(group['code'].tolist())
            result[trade_date] = stocks
        return result
    
    def _get_shenwan_classify(self):
        """
        è·å–ç”³ä¸‡åˆ†ç±»æ•°æ®
        """
        sw_l1 = jq.get_industries(name='sw_l1').reset_index(names='index_code')
        sw_l2 = jq.get_industries(name='sw_l2').reset_index(names='index_code')
        sw_l3 = jq.get_industries(name='sw_l3').reset_index(names='index_code')
        # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
        all_stock_codes = self.get_all_stocks()
        stock_industry_dict = jq.get_industry(all_stock_codes)
        stock_industry_df = pd.json_normalize(stock_industry_dict.values()).assign(stock_code=stock_industry_dict.keys()).reset_index(drop=True)

        sw_l1['start_date'] = sw_l1['start_date'].dt.strftime('%Y-%m-%d')
        sw_l2['start_date'] = sw_l2['start_date'].dt.strftime('%Y-%m-%d')
        sw_l3['start_date'] = sw_l3['start_date'].dt.strftime('%Y-%m-%d')
        self.save_data_to_database_with_threadpool(sw_l1,["index_code"],self.table_name_sw_l1)
        self.save_data_to_database_with_threadpool(sw_l2,["index_code"],self.table_name_sw_l2)
        self.save_data_to_database_with_threadpool(sw_l3,["index_code"],self.table_name_sw_l3)
        self.save_data_to_database_with_threadpool(stock_industry_df,["stock_code"],self.table_name_shenwan_stock_industry_df)
        print("\nğŸ“Š ç”³ä¸‡åˆ†ç±»æ•°æ®è·å–å®Œæˆ")

    def _get_shenwan_index(self):
        """
        è·å–ç”³ä¸‡æŒ‡æ•°æ•°æ®
        """
        all_market_indexes = jq.get_all_securities(types=['index'])  # æ— å‚æ•°=è¿”å›å…¨å¸‚åœºæ‰€æœ‰æŒ‡æ•°
        shenwan_index = all_market_indexes[all_market_indexes.index.str.startswith('80')]
        return shenwan_index
    
    def convert_jq_code_suffix(self,input_data, col_name: str = 'code', with_dot: bool = True):
        """
        è½¬æ¢èšå®½ï¼ˆJoinQuantï¼‰è‚¡ç¥¨ä»£ç åç¼€ï¼š.XSHGâ†’sh/.shï¼Œ.XSHEâ†’sz/.sz
        :param input_data: è¾“å…¥æ•°æ®ï¼Œæ”¯æŒ3ç§ç±»å‹ï¼š
                        - å•ä¸ªå­—ç¬¦ä¸²ï¼ˆå¦‚'600000.XSHG'ï¼‰
                        - pandas Seriesï¼ˆå¦‚df['code']ï¼‰
                        - pandas DataFrameï¼ˆéœ€æŒ‡å®šcol_nameï¼‰
        :param col_name: ä»…DataFrameæ—¶ç”Ÿæ•ˆï¼ŒæŒ‡å®šè¦å¤„ç†çš„åˆ—åï¼Œé»˜è®¤'code'
        :param with_dot: åç¼€æ˜¯å¦å¸¦ç‚¹ï¼Œé»˜è®¤Falseï¼ˆå¦‚600000shï¼‰ï¼›Trueåˆ™ä¸º600000.sh
        :return: å¤„ç†åçš„ç»“æœï¼ˆåŒè¾“å…¥ç±»å‹ï¼šå­—ç¬¦ä¸²/Series/DataFrameï¼‰
        :raises TypeError: è¾“å…¥ç±»å‹ä¸æ”¯æŒæ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # å®šä¹‰æ›¿æ¢è§„åˆ™ï¼ˆæ ¹æ®with_dotè°ƒæ•´ï¼‰
        replace_rules = {
            '.XSHG': '.SH' if with_dot else 'SH',
            '.XSHE': '.SZ' if with_dot else 'SZ'
        }
        
        # å¤„ç†å•ä¸ªå­—ç¬¦ä¸²
        if isinstance(input_data, str):
            code = input_data
            for old_suffix, new_suffix in replace_rules.items():
                code = code.replace(old_suffix, new_suffix)
            return code
        
        # å¤„ç†pandas Series
        elif isinstance(input_data, pd.Series):
            series_data = input_data.copy()  # é¿å…ä¿®æ”¹åŸæ•°æ®
            for old_suffix, new_suffix in replace_rules.items():
                # regex=Falseï¼šç²¾å‡†åŒ¹é…å­—ç¬¦ä¸²ï¼Œé¿å….è¢«æ­£åˆ™è§£æ
                series_data = series_data.str.replace(old_suffix, new_suffix, regex=False)
            return series_data
        
        # å¤„ç†pandas DataFrame
        elif isinstance(input_data, pd.DataFrame):
            df_data = input_data.copy()  # é¿å…ä¿®æ”¹åŸæ•°æ®
            if col_name not in df_data.columns:
                raise ValueError(f"DataFrameä¸­ä¸å­˜åœ¨åˆ—åï¼š{col_name}ï¼Œè¯·æ£€æŸ¥col_nameå‚æ•°")
            
            # æ‰¹é‡æ›¿æ¢æŒ‡å®šåˆ—çš„åç¼€
            for old_suffix, new_suffix in replace_rules.items():
                df_data[col_name] = df_data[col_name].str.replace(old_suffix, new_suffix, regex=False)
            return df_data
    
        # ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹
        else:
            raise TypeError(
                f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹ï¼š{type(input_data)}ï¼\n"
                "è¯·è¾“å…¥ä»¥ä¸‹ç±»å‹ï¼šå•ä¸ªå­—ç¬¦ä¸² / pandas Series / pandas DataFrame"
            )

    def save_data_to_database_with_threadpool_old(self, data_df, primary_keys, table_name, max_workers=40, operation_type='update'):
        """
        ä½¿ç”¨çº¿ç¨‹æ± å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
        :param data_df: è¦ä¿å­˜çš„æ•°æ®æ¡†
        :param primary_keys_map: ä¸»é”®åˆ—è¡¨
        :param table_name: è¡¨å
        :param max_workers: çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        :param operation_type: æ“ä½œç±»å‹ ('update', 'replace', 'append')
        """
        def save_single_table(df, primary_keys, table_name):
            """å•ä¸ªè¡¨çš„ä¿å­˜å‡½æ•°"""
            if df is not None and not df.empty:
                try:
                    # ä½¿ç”¨save_df_to_clickhouseæ–¹æ³•ä¿å­˜æ•°æ®åˆ°ClickHouse
                    success = self.db.save_df_to_clickhouse(
                        df=df,
                        table_name=table_name,
                        primary_keys=primary_keys,
                        operation_type=operation_type
                    )
                    if success:
                        return True
                    else:
                        return False
                except Exception as e:
                    return False
            else:
                return False

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ•°æ®åº“ä¿å­˜æ“ä½œ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future = executor.submit(save_single_table, data_df, primary_keys, table_name)
            try:
                result = future.result()
                if result is False:
                    print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼{table_name} è¡¨")
            except Exception as e:
                print(f"âŒ å¤„ç† {table_name} è¡¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

    def save_data_to_database_with_threadpool(self, data_df, primary_keys, table_name, 
                                            max_workers=10, operation_type='update', 
                                            batch_size=10000, show_progress=False):
        """
        ä½¿ç”¨çº¿ç¨‹æ± å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼Œæ”¯æŒå¤§æ•°æ®é›†åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        :param data_df: è¦ä¿å­˜çš„æ•°æ®æ¡†
        :param primary_keys: ä¸»é”®åˆ—è¡¨
        :param table_name: è¡¨å
        :param max_workers: çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        :param operation_type: æ“ä½œç±»å‹ ('update', 'replace', 'append')
        :param batch_size: æ¯æ‰¹å¤„ç†çš„æ•°æ®é‡
        :param show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        :return: æ€»ä½“æˆåŠŸçŠ¶æ€
        """
        if data_df is None or data_df.empty:
            return True

        def save_single_batch(batch_df, batch_index):
            """
            ä¿å­˜å•ä¸ªæ•°æ®æ‰¹æ¬¡åˆ°æ•°æ®åº“
            :param batch_df: æ‰¹æ¬¡æ•°æ®
            :param batch_index: æ‰¹æ¬¡ç´¢å¼•
            :return: (æ‰¹æ¬¡ç´¢å¼•, æˆåŠŸçŠ¶æ€, æ•°æ®è¡Œæ•°)
            """
            try:
                if batch_df is not None and not batch_df.empty:
                    # ä½¿ç”¨save_df_to_clickhouseæ–¹æ³•ä¿å­˜æ•°æ®åˆ°ClickHouse
                    success = self.db.save_df_to_clickhouse(
                        df=batch_df,
                        table_name=table_name,
                        primary_keys=primary_keys,
                        operation_type=operation_type
                    )
                    return (batch_index, success, len(batch_df))
                else:
                    return (batch_index, True, 0)  # ç©ºæ•°æ®æ‰¹æ¬¡è§†ä¸ºæˆåŠŸ
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_index} ä¿å­˜å¤±è´¥: {str(e)[:100]}...")
                return (batch_index, False, len(batch_df))

        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        total_rows = len(data_df)
        num_batches = (total_rows + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´

        # åˆ†å‰²æ•°æ®ä¸ºå¤šä¸ªæ‰¹æ¬¡
        batches = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_rows)
            batch_df = data_df.iloc[start_idx:end_idx].copy()
            batches.append((batch_df, i))

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ•°æ®åº“ä¿å­˜æ“ä½œ
        results = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(save_single_batch, batch_df, batch_idx): batch_idx
                for batch_df, batch_idx in batches
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            if show_progress:
                progress_bar = tqdm(total=num_batches, desc=f"ğŸ’¾ ä¿å­˜åˆ° {table_name}", unit="æ‰¹")

            for future in as_completed(future_to_batch):
                batch_result = future.result()
                results.append(batch_result)
                if show_progress:
                    progress_bar.update(1)

            if show_progress:
                progress_bar.close()

        # ç»Ÿè®¡ç»“æœ
        total_success = sum(1 for r in results if r[1])
        total_failed = num_batches - total_success
        total_saved_rows = sum(r[2] for r in results if r[1])

        # å¦‚æœæœ‰å¤±è´¥æ‰¹æ¬¡ï¼Œå°è¯•é‡æ–°ä¿å­˜å¤±è´¥çš„æ‰¹æ¬¡
        if total_failed > 0:
            print(f"ğŸ”„ å°è¯•é‡æ–°ä¿å­˜ {total_failed} ä¸ªå¤±è´¥çš„æ‰¹æ¬¡...")
            failed_batches = [(batches[batch_idx][0], batch_idx) for batch_idx, success, _ in results if not success]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                retry_futures = {
                    executor.submit(save_single_batch, batch_df, batch_idx): batch_idx
                    for batch_df, batch_idx in failed_batches
                }

                retry_results = []
                if show_progress:
                    retry_progress = tqdm(total=len(failed_batches), desc=f"ğŸ”„ é‡è¯•ä¿å­˜åˆ° {table_name}", unit="æ‰¹")

                for future in as_completed(retry_futures):
                    retry_result = future.result()
                    retry_results.append(retry_result)
                    if show_progress:
                        retry_progress.update(1)

                if show_progress:
                    retry_progress.close()

            # ç»Ÿè®¡é‡è¯•ç»“æœ
            retry_success = sum(1 for r in retry_results if r[1])
            retry_failed = len(failed_batches) - retry_success
            retry_saved_rows = sum(r[2] for r in retry_results if r[1])

            print(f"\nğŸ”„ é‡è¯•ç»“æœç»Ÿè®¡ ({table_name}):")
            print(f"   é‡è¯•æ‰¹æ¬¡: {len(failed_batches)}")
            print(f"   é‡è¯•æˆåŠŸ: {retry_success}")
            print(f"   é‡è¯•å¤±è´¥: {retry_failed}")
            print(f"   é‡è¯•æˆåŠŸä¿å­˜è¡Œæ•°: {retry_saved_rows}")

            # æ›´æ–°æ€»ç»Ÿè®¡
            total_success += retry_success
            total_saved_rows += retry_saved_rows

        # æœ€ç»ˆç»“æœåˆ¤æ–­
        if total_success == num_batches:
            return True
        else:
            print(f"âš ï¸ éƒ¨åˆ†æ‰¹æ¬¡æ•°æ®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return False


    def update_A_basic(self, start_date_str, end_date_str):
        pass



